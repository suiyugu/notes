## Inference Attacks Against Graph Neural Networks

#### USENIX Security 2022

这篇文献主要研究了图嵌入的信息泄露问题，提出了三种推理攻击，以及一种基于图嵌入扰动的防御机制。

### 背景

现实系统很多相互之间有关联的数据都可以用图表示（非欧式数据

图数据有**非欧**几里德性质，不具备向量等数据类型的坐标、矢量空间等性质→分析计算困难

因此提出了**图嵌入算法** 将非欧氏空间中的图数据转化为低维向量 （欧氏空间 

###### 图嵌入：图神经网络聚合邻接节点信息 获得节点嵌入 节点嵌入(图池化)聚合为图嵌入

已证明了嵌入可以泄漏欧几里得空间中图像和文本数据的敏感信息

数据持有者共享图嵌入用于下游分析任务的安全和隐私问题：图嵌入是否会泄露图包含的敏感结构信息？

###### 本文：三种推理攻击→图嵌入信息泄露问题

![image-20231014114024314](https://gitee.com/sygu66/img/raw/master/image-20231014114024314.png)

######            提出了一种基于图嵌入摄动的防御机制

##### Threat Model

场景：攻击者从受害者那里获得**整个**图嵌入（目标图嵌入$H_{GT}$

存在来自目标图的相同分布的**辅助数据集**$D_{aux}$

攻击者只能黑盒访问目标嵌入模型

目的：推断出用于生成图嵌入的图的敏感信息。我们称此图为目标图$G_T$，用于生成目标图嵌入目标图的GNN模型为目标嵌入模型$F_T$

- 图嵌入模型由 节点嵌入 和图池化
- 使用图嵌入比以往的研究中使用节点嵌入更加困难，整个图被压缩为单个向量



##### massage passing  

获取节点嵌入

##### 图池化 

聚合节点嵌入 形成图嵌入$H_G=\sum(H_u,\forall u\in G)$

全局池化丢失图结构信息，分层池化

根据n个节点嵌入找到m个簇，其中1 < m < n。接下来，我们将每个簇视为一个节点，特征为该簇的图嵌入，然后迭代地应用消息传递和聚类操作，直到只有一个图嵌入。

如何学习簇分配矩阵？

[^聚类分配矩阵]:矩阵的元素（i，j）表示第 i 个数据点属于第 j 个聚类的概率或权重。通常，这些权重是非负的，且每行的权重之和等于1。

differential pooling  辅助的链路预测目标

mincut pooling  最小切割目标



#### **Property Inference Attack**

###### 准确推断目标图基本属性（节点数、边数、图密度）

比如：

$\digamma_{AP}:H_{G_{T}}→\{ graph\ properties \}$

![image-20231014115755696](https://gitee.com/sygu66/img/raw/master/image-20231014115755696.png)

攻击模型组成：特征提取器＋输出层同步预测

辅助数据集（属性值域离散化处理）训练攻击模型

将属性域**划分**（连续数据离散化 ，降低了数据的复杂性，减少数据噪声，更适合机器学习方法（可能会导致信息损失

![image-20231023004647832](F:\imgs\image-20231023004647832.png)

#### 实验

- 数据集：评估GNN模型性能的基准数据集  DD, ENZYMES,AIDS, NCI1, and OVCAR-8H
- 图嵌入模型组成：三层SAGE＋三种图池化 以图池化方法命名图嵌入模型
- 采用**随机猜测**（因不同的分类方案而异）和**直接总结辅助数据集**（e.g.均值）作为基线攻击方法。



攻击设定：

推断图属性：节点数、边数、图密度、图直径和图半径。 

对照组：Random和Baseline方法分别表示随机猜测和汇总辅助数据集

![image-20231015205236992](https://gitee.com/sygu66/img/raw/master/image-20231015205236992.png)

- 列：数据集 行：要推断的图属性 图例：图嵌入模型 组：桶化方案（ ==For the graph density, the property domain is [0:0;1:0].？？？==
- k越大导致攻击精度变差：需要高粒度图结构信息
- meanpool攻击精度最差，甚至接近随即猜测：直接平均节点嵌入丢失图结构信息

![image-20231015205329400](https://gitee.com/sygu66/img/raw/master/image-20231015205329400.png)

- 验证了数据的可移植性，即辅助数据集与目标图有着不同的图分布（OVC和MOL之间以及NCI1和PC3之间



#### **Subgraph Inference Attack**

$\digamma_{AS}:\lang H_{G{T}},G_{S} \rang \rightarrow \{ G_{S} \in G_{T},G_{S} \notin G_{T} \}$

![image-20231014115823472](https://gitee.com/sygu66/img/raw/master/image-20231014115823472.png)

目的：高置信度确定子图是否包含于目标图中（DD数据集实现0.98的攻击AUC）

假设：<u>子图需要是目标图的主体部分</u>？（实验中使用的是含目标图20%-80%节点的子图）

（信息集中 攻击效率高？价值大？）

难点：

1. 图嵌入/向量和感兴趣子图/图格式不同，无法直接比较
2. 子图同构问题完全NP

设计了一种新的图嵌入提取器，使子图推理攻击模型能够同时从图嵌入和感兴趣的子图中学习。

子图转化为图嵌入→与目标图嵌入聚合→二元分类器预测（MLP）

训练模型：

- 通过**图采样方法**生成子图正负样本（） 训练分类器 这里有两个辅助图 正负

  [^图采样方法]: 随机游走 雪球 森林火（？

- 聚合两种目标和子图嵌入的方法：连接 元素差异 欧几里得距离（对比试验）

训练攻击模型：使用交叉熵损失和梯度下降算法。二元分类器和图嵌入提取器是同时训练的（强调这个干嘛）



AUC值被广泛用于衡量阈值范围内的二值分类性能

对比：

- 将子图转换为子图嵌入。在攻击模型中对嵌入提取器和二值分类器进行联合训练
- 子图嵌入直接从目标模型生成，同时还有目标图嵌入。训练一个孤立的二分类器作为攻击模型

##### ![image-20231015213314286](https://gitee.com/sygu66/img/raw/master/image-20231015213314286.png)

行：数据集 列：不同采样方法（随机游走、雪球、森林火灾）图例：图嵌入模型 组：采样比例

- 实验证明了图嵌入提取器在攻击模型中的必要性：与从目标模型获得子图嵌入的基线攻击进行比较。
- 对于不同的嵌入模型，MeanPool最好，与属性推理攻击相反。（可能）<u>因为DiffPool和MinCutPool分解了图结构，加大了图匹配的难度。</u>

![image-20231015213354889](https://gitee.com/sygu66/img/raw/master/image-20231015213354889.png)

![image-20231015215531719](https://gitee.com/sygu66/img/raw/master/image-20231015215531719.png)

类似的可移植性验证实验。（抽样方法＋图嵌入模型）

![image-20231015215305523](https://gitee.com/sygu66/img/raw/master/image-20231015215305523.png)

对于聚合（子图嵌入和目标图嵌入）的方法进行了比较，element-wise difference表现最好（该方法提取到了更多两者之间的差异信息量），concatenation的AUC接近随即猜测

#### **Graph Reconstruction Attack**

###### 重建与目标图具有相似图结构（度分布，局部聚类系数等）的图（邻接矩阵

$\digamma_{AR}:H_{G_{T}} \rightarrow G_{R}$

![image-20231014115853254](https://gitee.com/sygu66/img/raw/master/image-20231014115853254.png)

由单向量重建全图

训练定制的图自编码器 然后用解码器作为攻击模型

E可为GNN D可为多层感知器 转化为邻接矩阵形式的图结构

图形自编码器和图像自编码器相比 增加了组件：<u>图形匹配 最大池化法</u> （原因：$G_R$、$G_{aux}$都没有节点排序< 邻接矩阵不同导致损失较大，Eloss=0

图自编码器$E$来进行图重构攻击。图自编码器训练完成后，将其解码器$D$作为攻击模型。

训练完后将目标图嵌入输入解码器，解码器将输出与目标图具有相似结构的重构图。

缺陷：训练中的图匹配环节复杂度达到了O(n^4) 仅适用于几十个节点（生物图 分子图）

未来：适用于更大的图 并同时重建节点特征，而非仅限于图结构



**实验：**

- 衡量**图同构性**和**宏观级别的图统计信息**
- WL算法（基本思想是迭代计算两个图的WL图核） 
- 介数中心性分布 局部聚类系数分布 度分布和紧密中心性分布
- 三个度量来度量目标图GT与重构图GR之间的分布相似度:余弦相似度、Wasserstein距离和Jensen-Shannon (JS)散度。直观上，较高的余弦相似度和较低的Wasserstein距离/JS散度意味着更好的攻击性能。

[^图同构性]: 图同构NP 近似 采用DGL实现的WL算法
[^宏观级别的图统计信息]: 度分布（Degree distribution）、局部聚类系数（Local Clustering Coefficient，LCC）、介数中心性（Betweenness Centrality，BC）和紧密中心性（Closeness Centrality，CC）

![image-20231016113220312](https://gitee.com/sygu66/img/raw/master/image-20231016113220312.png)

![image-20231016113153418](https://gitee.com/sygu66/img/raw/master/image-20231016113153418.png)

从图同构和宏观级图统计(通过余弦相似度度量)的角度说明了攻击性能。、

![image-20231016114015265](https://gitee.com/sygu66/img/raw/master/image-20231016114015265.png)

使用不同时期训练的图自动编码器进行实验。随着训练时期的增加，攻击性能也增加→图自动编码器的质量对攻击有积极的影响，但迭代十次后收益不大。

#### 基于图嵌入扰动的有效防御机制

主要思想是在与第三方共享之前，在图嵌入中添加拉普拉斯噪声。

与$H^{\sim}_{G_{T}} = H_{G_T} + Lap(\beta)$

$Lap(\beta)$表示从具有尺度参数$\beta$的拉普拉斯分布中采样的随机变量：$P_r [Lap(\beta)=x]= \frac{1}{2 \beta} e^-{\frac{|x|}{\beta}}$

当加入不同程度的噪声时，防御效果提升也对应着信息结构被破坏，也就是准确率下降，所以要选择适度的噪音水平

###### 在不显著降低图分类任务性能的情况下**减轻推理攻击**

![image-20231016113522725](https://gitee.com/sygu66/img/raw/master/image-20231016113522725.png)

这篇论文研究了这种防御机制在子图推理攻击和属性推理攻击上不同拉普拉斯噪声时令其攻击性能的衰减，以及在对应参数时图分类任务准确性的下降

前两列分别表示属性推理和子图推理的攻击性能，最后一列表示正态图分类任务的准确率。

每幅图中，x轴表示拉普拉斯噪声的缩放参数b, b越大噪声级越高。y轴表示攻击性能/正态图分类精度。



### 结论

攻击有效 且不随辅助数据集结构是否与目标图相同影响

防御机制有效缓解推理攻击 且不出现明显性能下降



多个数据集

数据可移植性

连续数据离散化

基线对比

利弊 

对比某组件存在的必要性（消融实验）

